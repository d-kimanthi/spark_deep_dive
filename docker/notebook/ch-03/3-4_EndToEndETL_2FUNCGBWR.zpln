{
  "paragraphs": [
    {
      "text": "%md\n## Chapter 3: Exercise 3-4: Writing your First Spark ETL\nThis notebook is a full end to end ETL example.\n1. We will read the coffee data into Spark using our custom `StructType`\n2. We will then use `coalesce(1)` to remove additional partitions and save our data in exactly one file\n3. Then we will output the results as a `Parquet` file: which is a strictly typed, hyper structured data format.\n4. Lastly, we will read our resulting parquet file back in and see if the results are what we expected them to be.\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-31 19:54:03.793",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eChapter 3: Exercise 3-4: Writing your First Spark ETL\u003c/h2\u003e\n\u003cp\u003eThis notebook is a full end to end ETL example.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe will read the coffee data into Spark using our custom \u003ccode\u003eStructType\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eWe will then use \u003ccode\u003ecoalesce(1)\u003c/code\u003e to remove additional partitions and save our data in exactly one file\u003c/li\u003e\n\u003cli\u003eThen we will output the results as a \u003ccode\u003eParquet\u003c/code\u003e file: which is a strictly typed, hyper structured data format.\u003c/li\u003e\n\u003cli\u003eLastly, we will read our resulting parquet file back in and see if the results are what we expected them to be.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611017093358_1451525536",
      "id": "paragraph_1610923294026_749698639",
      "dateCreated": "2021-01-19 00:44:53.358",
      "dateStarted": "2021-01-31 19:54:03.809",
      "dateFinished": "2021-01-31 19:54:03.827",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n# Load schema from JSON file\nwith open(\u0027/learn/coffee_schema.json\u0027, \u0027r\u0027) as f:\n    schema_dict \u003d json.load(f)\n    print(type(schema_dict))\n    coffee_schema_restored \u003d T.StructType.fromJson(schema_dict)\n    print(coffee_schema_restored)",
      "user": "anonymous",
      "dateUpdated": "2025-11-23 00:41:10.257",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cclass \u0027dict\u0027\u003e\nStructType([StructField(\u0027name\u0027, StringType(), True), StructField(\u0027roast\u0027, DoubleType(), True)])\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1763856499938_1361644980",
      "id": "paragraph_1763856499938_1361644980",
      "dateCreated": "2025-11-23 00:08:19.940",
      "dateStarted": "2025-11-23 00:41:10.268",
      "dateFinished": "2025-11-23 00:41:10.707",
      "status": "FINISHED"
    },
    {
      "title": "Read, Transform and Write (ETL)",
      "text": "%pyspark\n\nspark.read.option(\"inferSchema\", \"false\")\\\n    .schema(coffee_schema_restored)\\\n    .csv(\"file:///learn/raw-coffee.txt\")\\\n    .write\\\n    .format(\"parquet\")\\\n    .mode(\"overwrite\")\\\n    .save(\"file:///learn/coffee.parquet\")",
      "user": "anonymous",
      "dateUpdated": "2025-11-23 00:53:45.897",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d35"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611017093358_204766208",
      "id": "paragraph_1610923180042_1100920629",
      "dateCreated": "2021-01-19 00:44:53.358",
      "dateStarted": "2025-11-23 00:53:45.933",
      "dateFinished": "2025-11-23 00:53:49.569",
      "status": "FINISHED"
    },
    {
      "title": "Read our Parquet Results",
      "text": "%pyspark\n\nspark.read.parquet(\"file:///learn/coffee.parquet\").createOrReplaceTempView(\"coffee_vw\")\n\nspark.sql(\"desc coffee_vw\").show(5)\nspark.sql(\"select * from coffee_vw\").show()",
      "user": "anonymous",
      "dateUpdated": "2025-11-23 00:42:01.996",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------+---------+--------------------+\n|col_name|data_type|             comment|\n+--------+---------+--------------------+\n|    name|   string|Name of the coffe...|\n|   roast|   double|Roast level from ...|\n+--------+---------+--------------------+\n\n+-----------+-----+\n|       name|roast|\n+-----------+-----+\n|    folgers| 10.0|\n|      yuban| 10.0|\n|  nespresso| 10.0|\n|     ritual|  4.0|\n|four barrel|  5.0|\n+-----------+-----+\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d33"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d34"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611017093359_228195739",
      "id": "paragraph_1610924951387_1045173537",
      "dateCreated": "2021-01-19 00:44:53.359",
      "dateStarted": "2025-11-23 00:42:02.002",
      "dateFinished": "2025-11-23 00:42:04.372",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\ndf \u003d spark.read.parquet(\"file:///learn/coffee.parquet\").toDF(\"name\",\"roast\")\ndf.rdd.getNumPartitions()",
      "user": "anonymous",
      "dateUpdated": "2025-11-23 00:37:15.344",
      "progress": 100,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "1"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d27"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611017986282_1094491072",
      "id": "paragraph_1611017986282_1094491072",
      "dateCreated": "2021-01-19 00:59:46.282",
      "dateStarted": "2025-11-23 00:37:15.351",
      "dateFinished": "2025-11-23 00:37:16.946",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-01-31 19:54:07.869",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1611018789843_2124518909",
      "id": "paragraph_1611018789843_2124518909",
      "dateCreated": "2021-01-19 01:13:09.844",
      "status": "FINISHED"
    }
  ],
  "name": "3-4_EndToEndETL",
  "id": "2FUNCGBWR",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}